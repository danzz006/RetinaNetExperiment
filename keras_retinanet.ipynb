{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxS0seeABAwj"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Object detection a very important problem in computer\n",
    "vision. Here the model is tasked with localizing the objects present in an\n",
    "image, and at the same time, classifying them into different categories.\n",
    "Object detection models can be broadly classified into \"single-stage\" and\n",
    "\"two-stage\" detectors. Two-stage detectors are often more accurate but at the\n",
    "cost of being slower. Here in this example, we will implement RetinaNet,\n",
    "a popular single-stage detector, which is accurate and runs fast.\n",
    "RetinaNet uses a feature pyramid network to efficiently detect objects at\n",
    "multiple scales and introduces a new loss, the Focal loss function, to alleviate\n",
    "the problem of the extreme foreground-background class imbalance.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [RetinaNet Paper](https://arxiv.org/abs/1708.02002)\n",
    "- [Feature Pyramid Network Paper](https://arxiv.org/abs/1612.03144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3VewK0CDLH3",
    "outputId": "364546ad-353f-458b-8389-0239e4d5282f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 24 13:45:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro GV100        Off  | 00000000:19:00.0 Off |                  Off |\n",
      "| 36%   44C    P2    25W / 250W |    273MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro GV100        Off  | 00000000:1A:00.0 Off |                  Off |\n",
      "| 44%   53C    P2    28W / 250W |     13MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro GV100        Off  | 00000000:67:00.0 Off |                  Off |\n",
      "| 41%   54C    P2    27W / 250W |     13MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro GV100        Off  | 00000000:68:00.0 Off |                  Off |\n",
      "| 64%   77C    P2   184W / 250W |  25536MiB / 32508MiB |     23%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1570      G   /usr/lib/xorg/Xorg                 63MiB |\n",
      "|    0   N/A  N/A      1825      G   /usr/bin/gnome-shell               24MiB |\n",
      "|    0   N/A  N/A      5839      G   /usr/lib/xorg/Xorg                 39MiB |\n",
      "|    0   N/A  N/A      5971      G   /usr/bin/gnome-shell              139MiB |\n",
      "|    1   N/A  N/A      1570      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A      5839      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      1570      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      5839      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      1570      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      5839      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A     26153      C   python                          25523MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "czb2cjMgBAwo",
    "outputId": "f3e1eb07-7794-426a-9e07-3ddb430fa279"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:45:04.362856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7wk4o1fEKqs",
    "outputId": "043d1a50-b25b-4e3c-95b6-6552a1d7e386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "-HkNjPQ3E1b0",
    "outputId": "d288ade1-51d0-45b8-f28d-6c7f339e6ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
      "\n",
      "\n",
      "Extracting from gdrive/MyDrive/validation.rar\n",
      "\n",
      "Extracting  validation.tfrecord                                          \b\b\b\b  3%\b\b\b\b  7%\b\b\b\b 11%\b\b\b\b 14%\b\b\b\b 18%\b\b\b\b 22%\b\b\b\b 25%\b\b\b\b 29%\b\b\b\b 33%\b\b\b\b 37%\b\b\b\b 40%\b\b\b\b 44%\b\b\b\b 48%\b\b\b\b 51%\b\b\b\b 55%\b\b\b\b 59%\b\b\b\b 63%\b\b\b\b 66%\b\b\b\b 70%\b\b\b\b 74%\b\b\b\b 77%\b\b\b\b 81%\b\b\b\b 85%\b\b\b\b 88%\b\b\b\b 92%\b\b\b\b 96%\b\b\b\b 99%\b\b\b\b\b  OK \n",
      "All OK\n"
     ]
    }
   ],
   "source": [
    "!unrar x gdrive/MyDrive/validation.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SEH0qdrBAww"
   },
   "source": [
    "## Downloading the COCO2017 dataset\n",
    "\n",
    "Training on the entire COCO2017 dataset which has around 118k images takes a\n",
    "lot of time, hence we will be using a smaller subset of ~500 images for\n",
    "training in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWALwboqBAxD",
    "outputId": "e25ed7dc-f675-40d3-879b-f96933372049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\n",
      "560529408/560525318 [==============================] - 7s 0us/step\n",
      "560537600/560525318 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
    "filename = os.path.join(os.getcwd(), \"data.zip\")\n",
    "keras.utils.get_file(filename, url)\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(\"data.zip\", \"r\") as z_fp:\n",
    "    z_fp.extractall(\"./\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_CHNAxWBAxE"
   },
   "source": [
    "## Implementing utility functions\n",
    "\n",
    "Bounding boxes can be represented in multiple ways, the most common formats are:\n",
    "\n",
    "- Storing the coordinates of the corners `[xmin, ymin, xmax, ymax]`\n",
    "- Storing the coordinates of the center and the box dimensions\n",
    "`[x, y, width, height]`\n",
    "\n",
    "Since we require both formats, we will be implementing functions for converting\n",
    "between the formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "It_7P62UBAxH"
   },
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd7CJXSiBAxR"
   },
   "source": [
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HTeLM1NZBAxW"
   },
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box, _cls, score in zip(boxes, classes, scores):\n",
    "        text = \"{}: {:.2f}\".format(_cls, score)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        ax.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            text,\n",
    "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
    "            clip_box=ax.clipbox,\n",
    "            clip_on=True,\n",
    "        )\n",
    "    plt.show()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCfm-Rv9BAxd"
   },
   "source": [
    "## Implementing Anchor generator\n",
    "\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0DD4ko7hBAxh"
   },
   "outputs": [],
   "source": [
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(3, 8)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(3, 8)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfo3lIu1BAxn"
   },
   "source": [
    "## Preprocessing data\n",
    "\n",
    "Preprocessing the images involves two steps:\n",
    "\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "\n",
    "Along with the images, bounding boxes are rescaled and flipped if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Op4kIVY3BAx3"
   },
   "outputs": [],
   "source": [
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack(\n",
    "            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n",
    "        )\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
    "):\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(\n",
    "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
    "    )\n",
    "    return image, image_shape, ratio\n",
    "\n",
    "\n",
    "def preprocess_data(sample):\n",
    "    \"\"\"Applies preprocessing step to a single sample\n",
    "\n",
    "    Arguments:\n",
    "      sample: A dict representing a single training sample.\n",
    "\n",
    "    Returns:\n",
    "      image: Resized and padded image with random horizontal flipping applied.\n",
    "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
    "        of the format `[x, y, width, height]`.\n",
    "      class_id: An tensor representing the class id of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "    image = sample[\"image\"]\n",
    "    bbox = swap_xy(sample[\"groundtruth_boxes\"])\n",
    "    class_id = tf.cast(sample[\"groundtruth_classes\"], dtype=tf.int32)\n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)\n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)\n",
    "    return image, bbox, class_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDfUIrItBAx6"
   },
   "source": [
    "## Encoding labels\n",
    "\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BLnxXvc9BAx8"
   },
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
    "    ):\n",
    "        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n",
    "\n",
    "        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n",
    "          to get a `(M, N)` shaped matrix.\n",
    "        2. The ground truth box with the maximum IOU in each row is assigned to\n",
    "          the anchor box provided the IOU is greater than `match_iou`.\n",
    "        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n",
    "          box is assigned with the background class.\n",
    "        4. The remaining anchor boxes that do not have any class assigned are\n",
    "          ignored during training.\n",
    "\n",
    "        Arguments:\n",
    "          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n",
    "            representing all the anchor boxes for a given input image shape,\n",
    "            where each anchor box is of the format `[x, y, width, height]`.\n",
    "          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n",
    "            the ground truth boxes, where each box is of the format\n",
    "            `[x, y, width, height]`.\n",
    "          match_iou: A float value representing the minimum IOU threshold for\n",
    "            determining if a ground truth box can be assigned to an anchor box.\n",
    "          ignore_iou: A float value representing the IOU threshold under which\n",
    "            an anchor box is assigned to the background class.\n",
    "\n",
    "        Returns:\n",
    "          matched_gt_idx: Index of the matched object\n",
    "          positive_mask: A mask for anchor boxes that have been assigned ground\n",
    "            truth boxes.\n",
    "          ignore_mask: A mask for anchor boxes that need to by ignored during\n",
    "            training\n",
    "        \"\"\"\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n",
    "        )\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            labels = labels.write(i, label)\n",
    "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
    "        return batch_images, labels.stack()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4JdhHrXBAyI"
   },
   "source": [
    "## Building the ResNet50 backbone\n",
    "\n",
    "RetinaNet uses a ResNet based backbone, using which a feature pyramid network\n",
    "is constructed. In the example we use ResNet50 as the backbone, and return the\n",
    "feature maps at strides 8, 16 and 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VO3dRbhRBAyK"
   },
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    \"\"\"Builds ResNet50 with pre-trained imagenet weights\"\"\"\n",
    "    backbone = keras.applications.ResNet50(\n",
    "        include_top=False, input_shape=[None, None, 3]\n",
    "    )\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n",
    "    ]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTSqR5OYBAyM"
   },
   "source": [
    "## Building Feature Pyramid Network as a custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SYT2h5yWBAyN"
   },
   "outputs": [],
   "source": [
    "class FeaturePyramid(keras.layers.Layer):\n",
    "    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset.\n",
    "      backbone: The backbone to build the feature pyramid from.\n",
    "        Currently supports ResNet50 only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone=None, **kwargs):\n",
    "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
    "        self.backbone = backbone if backbone else get_backbone()\n",
    "        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n",
    "        self.upsample_2x = keras.layers.UpSampling2D(2)\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
    "        p3_output = self.conv_c3_1x1(c3_output)\n",
    "        p4_output = self.conv_c4_1x1(c4_output)\n",
    "        p5_output = self.conv_c5_1x1(c5_output)\n",
    "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
    "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
    "        p3_output = self.conv_c3_3x3(p3_output)\n",
    "        p4_output = self.conv_c4_3x3(p4_output)\n",
    "        p5_output = self.conv_c5_3x3(p5_output)\n",
    "        p6_output = self.conv_c6_3x3(c5_output)\n",
    "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "        return p3_output, p4_output, p5_output, p6_output, p7_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oFXBH9EBAyP"
   },
   "source": [
    "## Building the classification and box regression heads.\n",
    "The RetinaNet model has separate heads for bounding box regression and\n",
    "for predicting class probabilities for the objects. These heads are shared\n",
    "between all the feature maps of the feature pyramid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jBoHqVKTBAyR"
   },
   "outputs": [],
   "source": [
    "def build_head(output_filters, bias_init):\n",
    "    \"\"\"Builds the class/box predictions head.\n",
    "\n",
    "    Arguments:\n",
    "      output_filters: Number of convolution filters in the final layer.\n",
    "      bias_init: Bias Initializer for the final convolution layer.\n",
    "\n",
    "    Returns:\n",
    "      A keras sequential model representing either the classification\n",
    "        or the box regression head depending on `output_filters`.\n",
    "    \"\"\"\n",
    "    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
    "    for _ in range(4):\n",
    "        head.add(\n",
    "            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n",
    "        )\n",
    "        head.add(keras.layers.ReLU())\n",
    "    head.add(\n",
    "        keras.layers.Conv2D(\n",
    "            output_filters,\n",
    "            3,\n",
    "            1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=kernel_init,\n",
    "            bias_initializer=bias_init,\n",
    "        )\n",
    "    )\n",
    "    return head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlZacANhBAyT"
   },
   "source": [
    "## Building RetinaNet using a subclassed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XtLAcHr3BAyT"
   },
   "outputs": [],
   "source": [
    "class RetinaNet(keras.Model):\n",
    "    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset.\n",
    "      backbone: The backbone to build the feature pyramid from.\n",
    "        Currently supports ResNet50 only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, backbone=None, **kwargs):\n",
    "        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n",
    "        self.fpn = FeaturePyramid(backbone)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(9 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=False):\n",
    "        features = self.fpn(image, training=training)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "        for feature in features:\n",
    "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
    "            cls_outputs.append(\n",
    "                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n",
    "            )\n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        return tf.concat([box_outputs, cls_outputs], axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qBto8YsBAyW"
   },
   "source": [
    "## Implementing a custom layer to decode predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fvRSe9oDBAyZ"
   },
   "outputs": [],
   "source": [
    "class DecodePredictions(tf.keras.layers.Layer):\n",
    "    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset\n",
    "      confidence_threshold: Minimum class probability, below which detections\n",
    "        are pruned.\n",
    "      nms_iou_threshold: IOU threshold for the NMS operation\n",
    "      max_detections_per_class: Maximum number of detections to retain per\n",
    "       class.\n",
    "      max_detections: Maximum number of detections to retain across all\n",
    "        classes.\n",
    "      box_variance: The scaling factors used to scale the bounding box\n",
    "        predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=80,\n",
    "        confidence_threshold=0.05,\n",
    "        nms_iou_threshold=0.5,\n",
    "        max_detections_per_class=100,\n",
    "        max_detections=100,\n",
    "        box_variance=[0.1, 0.1, 0.2, 0.2],\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(DecodePredictions, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.nms_iou_threshold = nms_iou_threshold\n",
    "        self.max_detections_per_class = max_detections_per_class\n",
    "        self.max_detections = max_detections\n",
    "\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n",
    "        boxes = box_predictions * self._box_variance\n",
    "        boxes = tf.concat(\n",
    "            [\n",
    "                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        boxes_transformed = convert_to_corners(boxes)\n",
    "        return boxes_transformed\n",
    "\n",
    "    def call(self, images, predictions):\n",
    "        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        box_predictions = predictions[:, :, :4]\n",
    "        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n",
    "        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "\n",
    "        return tf.image.combined_non_max_suppression(\n",
    "            tf.expand_dims(boxes, axis=2),\n",
    "            cls_predictions,\n",
    "            self.max_detections_per_class,\n",
    "            self.max_detections,\n",
    "            self.nms_iou_threshold,\n",
    "            self.confidence_threshold,\n",
    "            clip_boxes=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l-0DmrEBAya"
   },
   "source": [
    "## Implementing Smooth L1 loss and Focal Loss as keras custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "k02olqnjBAya"
   },
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred\n",
    "        )\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        loss = clf_loss + box_loss\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x90rbVSBAyc"
   },
   "source": [
    "## Setting up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1ipWgX_yBAyd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:45:34.729319: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-24 13:45:34.730496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-24 13:45:34.770874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:19:00.0 name: Quadro GV100 computeCapability: 7.0\n",
      "coreClock: 1.627GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 810.62GiB/s\n",
      "2022-03-24 13:45:34.772348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:1a:00.0 name: Quadro GV100 computeCapability: 7.0\n",
      "coreClock: 1.627GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 810.62GiB/s\n",
      "2022-03-24 13:45:34.773798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:67:00.0 name: Quadro GV100 computeCapability: 7.0\n",
      "coreClock: 1.627GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 810.62GiB/s\n",
      "2022-03-24 13:45:34.773844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-24 13:45:34.776965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-24 13:45:34.777043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-24 13:45:34.778549: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-24 13:45:34.778843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-24 13:45:34.780588: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-24 13:45:34.781385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-24 13:45:34.781596: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-24 13:45:34.789869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2\n",
      "2022-03-24 13:45:34.790541: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-24 13:45:34.791199: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-24 13:45:35.057074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:19:00.0 name: Quadro GV100 computeCapability: 7.0\n",
      "coreClock: 1.627GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 810.62GiB/s\n",
      "2022-03-24 13:45:35.061948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:1a:00.0 name: Quadro GV100 computeCapability: 7.0\n",
      "coreClock: 1.627GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 810.62GiB/s\n",
      "2022-03-24 13:45:35.064269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:67:00.0 name: Quadro GV100 computeCapability: 7.0\n",
      "coreClock: 1.627GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 810.62GiB/s\n",
      "2022-03-24 13:45:35.064319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-24 13:45:35.064357: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-24 13:45:35.064367: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-24 13:45:35.064377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-24 13:45:35.064388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-24 13:45:35.064397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-24 13:45:35.064407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-24 13:45:35.064416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-24 13:45:35.091275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2\n",
      "2022-03-24 13:45:35.091341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-24 13:45:36.074238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-24 13:45:36.074273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2 \n",
      "2022-03-24 13:45:36.074280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y Y \n",
      "2022-03-24 13:45:36.074285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N Y \n",
      "2022-03-24 13:45:36.074289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 2:   Y Y N \n",
      "2022-03-24 13:45:36.081407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29872 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:19:00.0, compute capability: 7.0)\n",
      "2022-03-24 13:45:36.084424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30117 MB memory) -> physical GPU (device: 1, name: Quadro GV100, pci bus id: 0000:1a:00.0, compute capability: 7.0)\n",
      "2022-03-24 13:45:36.087362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 30117 MB memory) -> physical GPU (device: 2, name: Quadro GV100, pci bus id: 0000:67:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"retinanet/\"\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "num_classes = 8\n",
    "# batch_size = 4\n",
    "\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n",
      "Number of devices: 3\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 6\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Icg0oLPDBAye"
   },
   "source": [
    "## Initializing and compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZLSUAdKBAye",
    "outputId": "af83b047-c4fe-4c46-c679-e9dade8aae50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():    \n",
    "    resnet50_backbone = get_backbone()\n",
    "    loss_fn = RetinaNetLoss(num_classes)\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
    "\n",
    "    model = RetinaNet(num_classes, resnet50_backbone)\n",
    "\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensorboard callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:46:32.663426: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-03-24 13:46:32.663474: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-03-24 13:46:32.663523: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 3 GPUs\n",
      "2022-03-24 13:46:32.664011: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcupti.so.11.0'; dlerror: libcupti.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64::/usr/local/cuda/lib64\n",
      "2022-03-24 13:46:32.665180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so\n",
      "2022-03-24 13:46:33.112317: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-03-24 13:46:33.117896: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHxJ9XM3BAyf"
   },
   "source": [
    "## Setting up callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MrfmaMrCBAyg"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tensorboard_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tb5WBJobBAyi"
   },
   "source": [
    "## Load the COCO2017 dataset using TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJmgufOkCbQk"
   },
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4lwe8DZVCLF7"
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "SM74-5M25gKn"
   },
   "outputs": [],
   "source": [
    "def _decode_image(parsed_tensors):\n",
    "  \"\"\"Decodes the image and set its static shape.\"\"\"\n",
    "  image = tf.io.decode_image(parsed_tensors['image/encoded'], channels=3)\n",
    "  image.set_shape([None, None, 3])\n",
    "  return image\n",
    "\n",
    "def _decode_boxes(parsed_tensors):\n",
    "  \"\"\"Concat box coordinates in the format of [ymin, xmin, ymax, xmax].\"\"\"\n",
    "  xmin = parsed_tensors['image/object/bbox/xmin']\n",
    "  xmax = parsed_tensors['image/object/bbox/xmax']\n",
    "  ymin = parsed_tensors['image/object/bbox/ymin']\n",
    "  ymax = parsed_tensors['image/object/bbox/ymax']\n",
    "  return tf.stack([xmin, ymin, xmax, ymax], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ktQOi6OCCsek"
   },
   "outputs": [],
   "source": [
    "val_image_dataset = tf.data.TFRecordDataset('validation.tfrecord')\n",
    "train_image_dataset = tf.data.TFRecordDataset('train.tfrecord')\n",
    "# train_image_dataset = tf.data.TFRecordDataset('train.tfrecord')\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'image/encoded':\n",
    "            tf.io.FixedLenFeature((), tf.string),\n",
    "        'image/source_id':\n",
    "            tf.io.FixedLenFeature((), tf.string),\n",
    "        'image/height':\n",
    "            tf.io.FixedLenFeature((), tf.int64),\n",
    "        'image/width':\n",
    "            tf.io.FixedLenFeature((), tf.int64),\n",
    "        'image/object/bbox/xmin':\n",
    "            tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax':\n",
    "            tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin':\n",
    "            tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax':\n",
    "            tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/label':\n",
    "            tf.io.VarLenFeature(tf.int64),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.train.Example proto using the dictionary above.\n",
    "  parsed_tensors =  tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "  for k in parsed_tensors:\n",
    "      if isinstance(parsed_tensors[k], tf.SparseTensor):\n",
    "        if parsed_tensors[k].dtype == tf.string:\n",
    "          parsed_tensors[k] = tf.sparse.to_dense(\n",
    "              parsed_tensors[k], default_value='')\n",
    "        else:\n",
    "          parsed_tensors[k] = tf.sparse.to_dense(\n",
    "              parsed_tensors[k], default_value=0)\n",
    "\n",
    "  image = _decode_image(parsed_tensors)\n",
    "  boxes = _decode_boxes(parsed_tensors)\n",
    "\n",
    "  decoded_tensors = {\n",
    "      'image': image,\n",
    "      'source_id': parsed_tensors['image/source_id'],\n",
    "      'height': parsed_tensors['image/height'],\n",
    "      'width': parsed_tensors['image/width'],\n",
    "      'groundtruth_classes': parsed_tensors['image/object/class/label'],\n",
    "      'groundtruth_boxes': boxes,\n",
    "    }\n",
    "  \n",
    "  return decoded_tensors\n",
    "\n",
    "val_dataset = val_image_dataset.map(_parse_image_function)\n",
    "train_dataset = train_image_dataset.map(_parse_image_function)\n",
    "# train_dataset = train_image_dataset.map(_parse_image_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "--bHS00GBAyj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(train_dataset, val_dataset), dataset_info = tfds.load(\\n    \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\"\\n)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  set `data_dir=None` to load the complete dataset\n",
    "'''\n",
    "(train_dataset, val_dataset), dataset_info = tfds.load(\n",
    "    \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vWgMM3fBAyk"
   },
   "source": [
    "## Setting up a `tf.data` pipeline\n",
    "\n",
    "To ensure that the model is fed with data efficiently we will be using\n",
    "`tf.data` API to create our input pipeline. The input pipeline\n",
    "consists for the following major processing steps:\n",
    "\n",
    "- Apply the preprocessing function to the samples\n",
    "- Create batches with fixed batch size. Since images in the batch can\n",
    "have different dimensions, and can also have different number of\n",
    "objects, we use `padded_batch` to the add the necessary padding to create\n",
    "rectangular tensors\n",
    "- Create targets for each sample in the batch using `LabelEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "g8B3AiI_BAyl"
   },
   "outputs": [],
   "source": [
    "autotune = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "    batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
    ")\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(autotune)\n",
    "\n",
    "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(\n",
    "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
    ")\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4cb6c1e2577660b0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4cb6c1e2577660b0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8090;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --port 8090 --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rzl_sXvmBAym"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1J43jGZABAym",
    "outputId": "d80e2e62-90dc-42ae-95ef-2807319e633b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:46:51.702269: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"IgnoreErrorsDataset/_28\"\n",
      "op: \"IgnoreErrorsDataset\"\n",
      "input: \"ParallelMapDatasetV2/_27\"\n",
      "attr {\n",
      "  key: \"log_warning\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 18\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-03-24 13:46:51.822237: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-03-24 13:46:51.853811: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 248 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 248 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:47:37.069629: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-24 13:47:44.117557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-24 13:47:44.897051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      1/Unknown - 58s 58s/step - loss: 3.5390 - accuracy: 0.2317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:47:52.147504: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-03-24 13:47:52.147539: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "      2/Unknown - 62s 4s/step - loss: 3.5141 - accuracy: 0.2300 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:47:54.246802: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-03-24 13:47:54.266708: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2022-03-24 13:47:54.492020: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 8312 callback api events and 8300 activity events. \n",
      "2022-03-24 13:47:54.734773: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-03-24 13:47:54.951693: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./logs/train/plugins/profile/2022_03_24_13_47_54\n",
      "2022-03-24 13:47:55.069421: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ./logs/train/plugins/profile/2022_03_24_13_47_54/nova44.trace.json.gz\n",
      "2022-03-24 13:47:55.376770: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./logs/train/plugins/profile/2022_03_24_13_47_54\n",
      "2022-03-24 13:47:55.394753: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ./logs/train/plugins/profile/2022_03_24_13_47_54/nova44.memory_profile.json.gz\n",
      "2022-03-24 13:47:55.410850: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/train/plugins/profile/2022_03_24_13_47_54Dumped tool data for xplane.pb to ./logs/train/plugins/profile/2022_03_24_13_47_54/nova44.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/train/plugins/profile/2022_03_24_13_47_54/nova44.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/train/plugins/profile/2022_03_24_13_47_54/nova44.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/train/plugins/profile/2022_03_24_13_47_54/nova44.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/train/plugins/profile/2022_03_24_13_47_54/nova44.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    484/Unknown - 771s 1s/step - loss: 3.6995 - accuracy: 0.2605"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:59:43.205402: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"IgnoreErrorsDataset/_23\"\n",
      "op: \"IgnoreErrorsDataset\"\n",
      "input: \"ParallelMapDatasetV2/_22\"\n",
      "attr {\n",
      "  key: \"log_warning\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: 5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484/484 [==============================] - 807s 2s/step - loss: 3.6993 - accuracy: 0.2605 - val_loss: 3.3578 - val_accuracy: 0.2863\n",
      "\n",
      "Epoch 00001: saving model to retinanet/weights_epoch_1\n",
      "Epoch 2/50\n",
      "484/484 [==============================] - 697s 1s/step - loss: 3.1843 - accuracy: 0.3342 - val_loss: 3.2637 - val_accuracy: 0.2927\n",
      "\n",
      "Epoch 00002: saving model to retinanet/weights_epoch_2\n",
      "Epoch 3/50\n",
      "484/484 [==============================] - 690s 1s/step - loss: 2.9530 - accuracy: 0.3327 - val_loss: 3.0957 - val_accuracy: 0.2918\n",
      "\n",
      "Epoch 00003: saving model to retinanet/weights_epoch_3\n",
      "Epoch 4/50\n",
      "484/484 [==============================] - 692s 1s/step - loss: 2.8528 - accuracy: 0.3388 - val_loss: 3.1100 - val_accuracy: 0.2903\n",
      "\n",
      "Epoch 00004: saving model to retinanet/weights_epoch_4\n",
      "Epoch 5/50\n",
      "484/484 [==============================] - 685s 1s/step - loss: 2.7742 - accuracy: 0.3407 - val_loss: 2.9827 - val_accuracy: 0.2977\n",
      "\n",
      "Epoch 00005: saving model to retinanet/weights_epoch_5\n",
      "Epoch 6/50\n",
      "484/484 [==============================] - 686s 1s/step - loss: 2.7187 - accuracy: 0.3409 - val_loss: 2.9999 - val_accuracy: 0.3013\n",
      "\n",
      "Epoch 00006: saving model to retinanet/weights_epoch_6\n",
      "Epoch 7/50\n",
      "484/484 [==============================] - 684s 1s/step - loss: 2.6611 - accuracy: 0.3364 - val_loss: 2.9694 - val_accuracy: 0.2970\n",
      "\n",
      "Epoch 00007: saving model to retinanet/weights_epoch_7\n",
      "Epoch 8/50\n",
      "484/484 [==============================] - 687s 1s/step - loss: 2.6309 - accuracy: 0.3371 - val_loss: 2.9133 - val_accuracy: 0.2981\n",
      "\n",
      "Epoch 00008: saving model to retinanet/weights_epoch_8\n",
      "Epoch 9/50\n",
      "484/484 [==============================] - 693s 1s/step - loss: 2.5743 - accuracy: 0.3407 - val_loss: 2.9264 - val_accuracy: 0.3017\n",
      "\n",
      "Epoch 00009: saving model to retinanet/weights_epoch_9\n",
      "Epoch 10/50\n",
      "484/484 [==============================] - 685s 1s/step - loss: 2.5286 - accuracy: 0.3407 - val_loss: 2.9452 - val_accuracy: 0.2955\n",
      "\n",
      "Epoch 00010: saving model to retinanet/weights_epoch_10\n",
      "Epoch 11/50\n",
      "484/484 [==============================] - 684s 1s/step - loss: 2.4825 - accuracy: 0.3397 - val_loss: 2.9268 - val_accuracy: 0.2985\n",
      "\n",
      "Epoch 00011: saving model to retinanet/weights_epoch_11\n",
      "Epoch 12/50\n",
      "484/484 [==============================] - 682s 1s/step - loss: 2.4296 - accuracy: 0.3414 - val_loss: 2.9729 - val_accuracy: 0.2905\n",
      "\n",
      "Epoch 00012: saving model to retinanet/weights_epoch_12\n",
      "Epoch 13/50\n",
      "484/484 [==============================] - 686s 1s/step - loss: 2.3854 - accuracy: 0.3409 - val_loss: 3.0230 - val_accuracy: 0.2965\n",
      "\n",
      "Epoch 00013: saving model to retinanet/weights_epoch_13\n",
      "Epoch 14/50\n",
      "292/484 [=================>............] - ETA: 4:30 - loss: 2.3270 - accuracy: 0.3409"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines, when training on full dataset\n",
    "# train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n",
    "# val_steps_per_epoch = \\\n",
    "#     dataset_info.splits[\"validation\"].num_examples // batch_size\n",
    "\n",
    "# train_steps = 4 * 100000\n",
    "# epochs = train_steps // train_steps_per_epoch\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# Running 100 training and 50 validation steps,\n",
    "# remove `.take` when training on the full dataset\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goBpcL2TBAyn"
   },
   "source": [
    "## Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fnaun852BAyn"
   },
   "outputs": [],
   "source": [
    "# Change this to `model_dir` when not using the downloaded weights\n",
    "weights_dir = \"data\"\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAJq6HrtBAyo"
   },
   "source": [
    "## Building inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-deZer6NBAyo"
   },
   "outputs": [],
   "source": [
    "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UucGgQbBAyq"
   },
   "source": [
    "## Generating detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDAWGr07BAyq"
   },
   "outputs": [],
   "source": [
    "def prepare_image(image):\n",
    "    image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "    return tf.expand_dims(image, axis=0), ratio\n",
    "\n",
    "\n",
    "val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n",
    "int2str = dataset_info.features[\"objects\"][\"label\"].int2str\n",
    "\n",
    "for sample in val_dataset.take(2):\n",
    "    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
    "    input_image, ratio = prepare_image(image)\n",
    "    detections = inference_model.predict(input_image)\n",
    "    num_detections = detections.valid_detections[0]\n",
    "    class_names = [\n",
    "        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
    "    ]\n",
    "    visualize_detections(\n",
    "        image,\n",
    "        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
    "        class_names,\n",
    "        detections.nmsed_scores[0][:num_detections],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of retinanet",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf2_clean",
   "language": "python",
   "name": "tf2_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
